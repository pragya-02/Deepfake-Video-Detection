{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xChpiF9aZJG"
      },
      "outputs": [],
      "source": [
        "#import packages\n",
        "import glob\n",
        "import shutil\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#HyperParams\n",
        "im_size = 224\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "seqlen = 10\n",
        "batchSize = 7\n",
        "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = \"cpu\"\n",
        "curr_epoch = 1\n",
        "random_state = 47\n",
        "path_to_model = 'ckpts/checkpoint.pt'\n",
        "video_files = glob.glob('/dataset/*.mp4')\n",
        "print(f'Total videos: {len(video_files)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "frame_count = []\n",
        "for video_file in video_files:\n",
        "  cap = cv2.VideoCapture(video_file)\n",
        "  if(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))<(seqlen+10)):\n",
        "    video_files.remove(video_file)\n",
        "    continue\n",
        "  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
        "print(\"Total no of video: \" , len(frame_count))\n",
        "print('Average frame per video:',np.mean(frame_count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Function to pad frames with black pixels\n",
        "def pad_frame(frame, target_size=(100, 100)):\n",
        "\n",
        "    height, width, _ = frame.shape\n",
        "\n",
        "    if width < target_size[1]:\n",
        "        left_padding = (target_size[0] - width) // 2\n",
        "        right_padding = target_size[0] - width - left_padding\n",
        "        frame = cv2.copyMakeBorder(frame, 0, 0, left_padding, right_padding, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
        "\n",
        "    if height < target_size[0]:\n",
        "        top_padding = (target_size[1] - height) // 2\n",
        "        bottom_padding = target_size[1] - height - top_padding\n",
        "        frame = cv2.copyMakeBorder(frame, top_padding, bottom_padding, 0, 0, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
        "\n",
        "    return cv2.resize(frame,target_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_list = {\n",
        "    \"top\" : [[164,172],[188,198],[282,292]],\n",
        "    \"nose_lips\" : [[202,242],[242,266]]\n",
        "\n",
        "    }\n",
        "\n",
        "def extract_individual_facial_parts(frame, nparr):\n",
        "\n",
        "    facial_parts_dict = {}\n",
        "\n",
        "    for idx in index_list:\n",
        "        concat = []\n",
        "        for i in index_list[idx]:\n",
        "            temp = nparr[i[0]:i[1]]\n",
        "            for val in temp:\n",
        "                concat.append(val)\n",
        "\n",
        "        f_set = concat\n",
        "        min_x, min_y = max_x, max_y = next(iter(f_set))\n",
        "        for point in f_set:\n",
        "            x, y = point\n",
        "            min_x = min(min_x, x)\n",
        "            max_x = max(max_x, x)\n",
        "            min_y = min(min_y, y)\n",
        "            max_y = max(max_y, y)\n",
        "\n",
        "        # Extreme diagonal vertices\n",
        "        min_x = int(min_x * 1)\n",
        "        min_x = max(0,min_x)\n",
        "        max_x = int(max_x * 1)\n",
        "        max_x = min(im_size,max_x)\n",
        "        min_y = int(min_y * 1)\n",
        "        min_y = max(0,min_y)\n",
        "        max_y = int(max_y * 1)\n",
        "        max_y = min(im_size,max_y)\n",
        "        x = (min_x, max_x)\n",
        "        y = (min_y, max_y)\n",
        "\n",
        "        temp = frame[y[0]:y[1], x[0]:x[1]]\n",
        "\n",
        "        if idx == \"top\":\n",
        "            # temp = pad_frame(temp,[100,100])\n",
        "            facial_parts_dict.update({\"top\" : temp})\n",
        "        elif idx == \"nose_lips\":\n",
        "            # temp = pad_frame(temp,[80,80])\n",
        "            facial_parts_dict.update({\"nose_lips\" : temp})\n",
        "\n",
        "    return facial_parts_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def combined_canvas(img1, img2):\n",
        "    \n",
        "    # Define the gap size\n",
        "    gap_size = 10  # Adjust as needed\n",
        "    \n",
        "    # Create a blank canvas to accommodate both images with the gap\n",
        "    canvas_height = img1.shape[0] + img2.shape[0] + gap_size\n",
        "    canvas_width = max(img1.shape[1], img2.shape[1])\n",
        "    canvas = np.zeros((canvas_height, canvas_width, 3), dtype=np.uint8)\n",
        "\n",
        "    # Calculate the vertical offset to align the second image in the middle\n",
        "    offset_y = (canvas_height - img1.shape[0] - img2.shape[0] - gap_size) // 2\n",
        "\n",
        "    # Copy img1 to the top portion of the canvas\n",
        "    canvas[:img1.shape[0], :img1.shape[1]] = img1\n",
        "\n",
        "    # Calculate the horizontal offset to align the second image in the middle\n",
        "    offset_x = (canvas_width - img2.shape[1]) // 2\n",
        "\n",
        "    # Copy img2 to the middle portion of the canvas\n",
        "    canvas[offset_y + img1.shape[0] + gap_size:offset_y + img1.shape[0] + img2.shape[0] + gap_size, offset_x:offset_x + img2.shape[1]] = img2\n",
        "\n",
        "\n",
        "    return pad_frame(canvas,[124,124])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the video name and labels from csv\n",
        "class video_dataset(Dataset):\n",
        "    def __init__(self,video_names,labels,blendshapes,facial_parts,sequence_length,transform = None, transform2 = None):\n",
        "        self.video_names = video_names\n",
        "        self.labels = labels\n",
        "        self.blendshapes = blendshapes\n",
        "        self.facial_parts = facial_parts\n",
        "        self.transform = transform\n",
        "        self.transform2 = transform2\n",
        "        self.count = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_names)\n",
        "    def __getitem__(self,idx):\n",
        "        video_path = self.video_names[idx]\n",
        "        frames = []\n",
        "        vid_blendshapes = []\n",
        "        vid_facial_parts = []\n",
        "        a = int(100/self.count)\n",
        "        first_frame = np.random.randint(0,a)\n",
        "        temp_video = video_path.split('/')[-1]\n",
        "        blendshapes_path = os.path.join(self.blendshapes, temp_video.split(\".\")[0])\n",
        "        facial_parts_path = os.path.join(self.facial_parts, temp_video.split(\".\")[0])\n",
        "        label = self.labels.iloc[(labels.loc[labels[\"file\"] == temp_video].index.values[0]),1]\n",
        "        if(label == 'FAKE'):\n",
        "          label = 0\n",
        "        if(label == 'REAL'):\n",
        "          label = 1\n",
        "        capture = cv2.VideoCapture(video_path)\n",
        "        frames_num = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        frames_num = min(frames_num,50)\n",
        "        for i in range(frames_num):\n",
        "          capture.grab()\n",
        "          success, frame = capture.retrieve()\n",
        "          if not success :\n",
        "            continue\n",
        "          temp_path_nparr = os.path.join(blendshapes_path,f\"{i+1}.npy\")\n",
        "          temp_path_nparr2 = os.path.join(facial_parts_path,f\"{i+1}.npy\")\n",
        "          if os.path.exists(temp_path_nparr) and os.path.exists(temp_path_nparr2):\n",
        "            frames.append(self.transform(frame))\n",
        "            temp_nparr = np.load(temp_path_nparr)\n",
        "            temp_narr2 = np.load(temp_path_nparr2)\n",
        "            facial_parts_temp = extract_individual_facial_parts(frame=frame,nparr=temp_narr2)\n",
        "            vid_blendshapes.append(torch.tensor(temp_nparr))\n",
        "            vid_facial_parts.append(self.transform2(combined_canvas(facial_parts_temp[\"top\"],facial_parts_temp[\"nose_lips\"])))\n",
        "          if(len(frames) == self.count):\n",
        "            break\n",
        "        frames = torch.stack(frames)\n",
        "        vid_blendshapes = torch.stack(vid_blendshapes)\n",
        "        vid_blendshapes = vid_blendshapes[:self.count]\n",
        "        vid_facial_parts_copy = torch.stack(vid_facial_parts[:self.count])\n",
        "        frames = frames[:self.count]\n",
        "\n",
        "        \n",
        "        return frames,vid_blendshapes,vid_facial_parts_copy,label\n",
        "\n",
        "def im_plot(tensor):\n",
        "    image = tensor.cpu().numpy().transpose(1,2,0)\n",
        "    b,g,r = cv2.split(image)\n",
        "    image = cv2.merge((r,g,b))\n",
        "    image = image*[0.22803, 0.22145, 0.216989] +  [0.43216, 0.394666, 0.37645]\n",
        "    image = image*255.0\n",
        "    plt.imshow(image.astype(int))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV4fSWcEbne2"
      },
      "outputs": [],
      "source": [
        "#generate list of fake and real videos\n",
        "def real_and_fake_videos_list(data_list):\n",
        "  header_list = [\"file\",\"label\",\"origfile\"]\n",
        "  lab = pd.read_csv('dataset/metadata.csv',names=header_list)\n",
        "  fake = []\n",
        "  real = []\n",
        "  for i in data_list:\n",
        "    temp_video = i.split('/')[-1]\n",
        "    label = lab.iloc[(labels.loc[labels[\"file\"] == temp_video].index.values[0]),1]\n",
        "    if(label == 'FAKE'):\n",
        "      fake.append(i)\n",
        "    if(label == 'REAL'):\n",
        "      real.append(i)\n",
        "  return real,fake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "FQa-RFLHb6tl",
        "outputId": "5eeb620c-58c5-4f82-ef39-e8ee77dfb0bd"
      },
      "outputs": [],
      "source": [
        "# load the labels and video in data loader\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "header_list = [\"file\",\"label\",\"origfile\"]\n",
        "labels = pd.read_csv('dataset/metadata.csv',names=header_list)\n",
        "blendshapes = os.path.join(\"blendshapes/\")\n",
        "facial_parts = os.path.join(\"facial_parts/\")\n",
        "# train_videos = video_files[:int(0.8*len(video_files))]\n",
        "# valid_videos = video_files[int(0.8*len(video_files)):]\n",
        "\n",
        "real, fake = real_and_fake_videos_list(video_files) \n",
        "temp = min(len(real), len(fake))\n",
        "if len(real) == temp:\n",
        "    fake = fake[:temp]\n",
        "if len(fake) == temp:\n",
        "    real = real[:temp]\n",
        "    \n",
        "real_train, real_valid, fake_train, fake_valid = train_test_split(real,fake, train_size=0.8, random_state=random_state) \n",
        "train_videos = real_train + fake_train\n",
        "valid_videos = real_valid + fake_valid\n",
        "\n",
        "print(\"train : \" , len(train_videos))\n",
        "print(\"test : \" , len(valid_videos))\n",
        "\n",
        "print(\"TRAIN: \", \"Real:\",len(real_train),\" Fake:\",len(fake_train))\n",
        "print(\"TEST: \", \"Real:\",len(real_valid),\" Fake:\",len(fake_valid))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "  return (\n",
        "      torch.stack([x[0] for x in batch]),\n",
        "      torch.stack([x[1] for x in batch]),\n",
        "      torch.stack([x[2] for x in batch]),\n",
        "      torch.tensor([x[3] for x in batch])\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Apply transformation and Dataloader\n",
        "\n",
        "train_transforms0 = transforms.Compose([\n",
        "        # create_train_transforms(),\n",
        "        transforms.ToPILImage(),                         \n",
        "        transforms.Resize((im_size,im_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean,std)\n",
        "       ])\n",
        "train_transforms_ccnn0 = transforms.Compose([\n",
        "        # create_train_transforms(),\n",
        "        transforms.ToPILImage(),                         \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean,std)\n",
        "       ])\n",
        "\n",
        "train_transforms1 = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomApply([\n",
        "        transforms.RandomRotation(15),  \n",
        "        transforms.ColorJitter(brightness=0.15),  \n",
        "    ], p=1),  \n",
        "    \n",
        "    transforms.Resize((im_size, im_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "train_transforms_ccnn1 = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomApply([\n",
        "        transforms.RandomRotation(15),  \n",
        "        transforms.ColorJitter(brightness=0.15),  \n",
        "    ], p=1),  \n",
        "    \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "        # create_val_transforms(),\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((im_size,im_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean,std)\n",
        "        ])\n",
        "\n",
        "test_transforms_ccnn = transforms.Compose([\n",
        "        # create_val_transforms(),\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean,std)\n",
        "        ])\n",
        "\n",
        "train_data0 = video_dataset(train_videos,labels,blendshapes,facial_parts,sequence_length = seqlen,transform = train_transforms0, transform2 = train_transforms_ccnn0)\n",
        "train_data1 = video_dataset(train_videos,labels,blendshapes,facial_parts,sequence_length = seqlen,transform = train_transforms1, transform2 = train_transforms_ccnn1)\n",
        "val_data = video_dataset(valid_videos,labels,blendshapes,facial_parts,sequence_length = seqlen,transform = test_transforms,transform2 = test_transforms_ccnn)\n",
        "train_data = train_data0 + train_data1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data,batch_size = batchSize,shuffle = True, num_workers = 8, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(val_data,batch_size = batchSize,shuffle = True, num_workers = 8, collate_fn=collate_fn)\n",
        "\n",
        "image,vid_blendshapes,vid_facial_parts,label = train_data[1239]\n",
        "\n",
        "im_plot(vid_facial_parts[2,:,:,:])        \n",
        "im_plot(image[0,:,:,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torchvision import models\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes, lstm_hidden_dim=2612, bidirectional=True):\n",
        "        super(Model, self).__init__()\n",
        "        model = models.resnext50_32x4d(pretrained = True)\n",
        "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)  # AdaptiveAvgPool2d to replace avgpool\n",
        "        self.lstm = nn.LSTM(lstm_hidden_dim, lstm_hidden_dim, bidirectional=bidirectional, batch_first=True)\n",
        "        self.dense1 = nn.Linear(lstm_hidden_dim * (2 if bidirectional else 1), 512)\n",
        "        self.dense2=nn.Linear(512, 2)\n",
        "        self.dp1 = nn.Dropout(0.4)\n",
        "        self.dp2 = nn.Dropout(0.4)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.facenet = InceptionResnetV1(pretrained='vggface2')\n",
        "\n",
        "    def forward(self, x, y, p1):\n",
        "        batch_size, seq_length, c, h, w = x.shape\n",
        "        batch_size, seq_length, c, h1, w1 = p1.shape\n",
        "\n",
        "\n",
        "        x = x.view(batch_size * seq_length, c, h, w)\n",
        "        p1 = p1.view(batch_size * seq_length, c, h1, w1)\n",
        "\n",
        "\n",
        "        p1 = self.facenet(p1)\n",
        "\n",
        "\n",
        "        x = self.model(x)\n",
        "        x = self.avgpool(x).flatten(1)\n",
        "\n",
        "        x = x.view(batch_size, seq_length, -1)  # Flatten x\n",
        "        y = y.view(batch_size, seq_length, -1)  # Flatten x\n",
        "        p1 = p1.view(batch_size, seq_length, -1)  # Flatten x\n",
        "\n",
        "        \n",
        "        x = torch.cat((x, y, p1), dim=2)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        lstm_out = lstm_out.contiguous().view(batch_size * seq_length, -1)\n",
        "        lstm_out = lstm_out.view(batch_size, seq_length, -1) \n",
        "        lstm_out = lstm_out[:, -1, :]  \n",
        "        lstm_out = self.dp1(lstm_out) #dropout layer 1\n",
        "        output1 = self.relu(self.dense1(lstm_out))\n",
        "        output1 = self.dp2(output1) #dropout layer 2\n",
        "        output=self.sigmoid(self.dense2(output1))\n",
        "        return 0, output\n",
        "\n",
        "# Example usage\n",
        "model = Model(2).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI-DEJOicewM"
      },
      "outputs": [],
      "source": [
        "#model train test functions\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "import sys\n",
        "longTensor = torch.cuda.LongTensor if device != \"cpu\" else torch.LongTensor\n",
        "floatTensor = torch.cuda.FloatTensor if device != \"cpu\" else torch.FloatTensor\n",
        "def train_epoch(curr_epoch, num_epochs, data_loader, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    trlosses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "    t = []\n",
        "    print(f\"[Epoch {curr_epoch}/{num_epochs}]\")\n",
        "    print(\"TRAIN\")\n",
        "    for i, (inputs,blendshapes_data,p1, targets) in enumerate(data_loader):\n",
        "        if torch.cuda.is_available() :\n",
        "            targets = targets.type(floatTensor)\n",
        "            blendshapes_data = blendshapes_data.type(floatTensor)\n",
        "            inputs = inputs.to(device)\n",
        "            p1 = p1.to(device)\n",
        "        _,outputs = model(inputs,blendshapes_data,p1)\n",
        "        loss  = criterion(outputs,targets.type(longTensor))\n",
        "        acc = calculate_accuracy(outputs, targets.type(longTensor))\n",
        "        trlosses.update(loss.item(), inputs.size(0))\n",
        "        accuracies.update(acc, inputs.size(0))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        sys.stdout.write(\n",
        "                \"\\r[Batch %d / %d] [Loss: %f, Acc: %.2f%%]\"\n",
        "                % (\n",
        "                    i,\n",
        "                    len(data_loader),\n",
        "                    trlosses.avg,\n",
        "                    accuracies.avg))\n",
        "   \n",
        "    return model, optimizer, criterion, trlosses.avg, accuracies.avg\n",
        "\n",
        "def test(curr_epoch,model, data_loader ,criterion):\n",
        "    print(\"\\nTEST\")\n",
        "    model.eval()\n",
        "    tlosses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "    pred = []\n",
        "    true = []\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs,blendshapes_data,p1, targets) in enumerate(data_loader):\n",
        "            if torch.cuda.is_available():\n",
        "                targets = targets.to(device).type(floatTensor)\n",
        "                blendshapes_data = blendshapes_data.type(floatTensor)\n",
        "                inputs = inputs.to(device)\n",
        "                p1 = p1.to(device)\n",
        "            _,outputs = model(inputs,blendshapes_data,p1)\n",
        "            loss = torch.mean(criterion(outputs, targets.type(longTensor)))\n",
        "            acc = calculate_accuracy(outputs,targets.type(longTensor))\n",
        "            _,p = torch.max(outputs,1)\n",
        "            true += (targets.type(longTensor)).detach().cpu().numpy().reshape(len(targets)).tolist()\n",
        "            pred += p.detach().cpu().numpy().reshape(len(p)).tolist()\n",
        "            tlosses.update(loss.item(), inputs.size(0))\n",
        "            accuracies.update(acc, inputs.size(0))\n",
        "            sys.stdout.write(\n",
        "                    \"\\r[Batch %d / %d]  [Loss: %f, Acc: %.2f%%]\"\n",
        "                    % (\n",
        "                        i,\n",
        "                        len(data_loader),\n",
        "                        tlosses.avg,\n",
        "                        accuracies.avg\n",
        "                        )\n",
        "                    )\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return true, pred, tlosses.avg, accuracies.avg\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    batch_size = targets.size(0)\n",
        "\n",
        "    _, pred = outputs.topk(1, 1, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(targets.view(1, -1))\n",
        "    n_correct_elems = correct.float().sum().item()\n",
        "    return 100* n_correct_elems / batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C88lw8zQctFz"
      },
      "outputs": [],
      "source": [
        "#confusion matrix plot and save function\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def print_confusion_matrix(y_true, y_pred,dirpath):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print('True positive = ', cm[0][0])\n",
        "    print('False positive = ', cm[0][1])\n",
        "    print('False negative = ', cm[1][0])\n",
        "    print('True negative = ', cm[1][1])\n",
        "    print('\\n')\n",
        "    df_cm = pd.DataFrame(cm, range(2), range(2))\n",
        "    sn.set(font_scale=1.4) # for label size\n",
        "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='d') # font size\n",
        "    plt.ylabel('Actual label', size = 20)\n",
        "    plt.xlabel('Predicted label', size = 20)\n",
        "    plt.xticks(np.arange(2), ['Fake', 'Real'], size = 16)\n",
        "    plt.yticks(np.arange(2), ['Fake', 'Real'], size = 16)\n",
        "    plt.ylim([2, 0])\n",
        "    plotpath = f'{dirpath}/confmatrix.png'\n",
        "    plt.savefig(plotpath, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    calculated_acc = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+ cm[1][1])\n",
        "    print(\"Calculated Accuracy\",calculated_acc*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7dvye-6cxBz"
      },
      "outputs": [],
      "source": [
        "#graph plot and save functions cell\n",
        "def plot_loss(train_loss_avg,test_loss_avg,num_epochs, dirpath):\n",
        "  loss_train = train_loss_avg\n",
        "  loss_val = test_loss_avg\n",
        "  epochs = range(1,num_epochs+1)\n",
        "  plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "  plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "  plt.title('Training and Validation loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plotpath = f'{dirpath}/lossplot.png'\n",
        "  plt.savefig(plotpath, bbox_inches='tight')\n",
        "  plt.show()\n",
        "def plot_accuracy(train_accuracy,test_accuracy,num_epochs, dirpath):\n",
        "  loss_train = train_accuracy\n",
        "  loss_val = test_accuracy\n",
        "  minm = (min(loss_train) - 10) if min(loss_train) <= min(loss_val) else (min(loss_val) - 10)\n",
        "  epochs = range(1,num_epochs+1)\n",
        "  plt.ylim(minm, 100)\n",
        "  plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
        "  plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
        "  plt.title('Training and Validation accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "  plotpath = f'{dirpath}/accplot.png'\n",
        "  plt.savefig(plotpath, bbox_inches='tight')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load ckpts\n",
        "from os.path import exists\n",
        "lr = 0.00001 \n",
        "weightDecay = 0.00001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= lr,weight_decay = weightDecay)\n",
        "# class_weights = torch.tensor([0.12, 0.88])\n",
        "# criterion = nn.CrossEntropyLoss(weight= class_weights).cuda()\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "train_loss_avg =[]\n",
        "train_accuracy = []\n",
        "test_loss_avg = []\n",
        "test_accuracy = []\n",
        "print(exists(path_to_model))\n",
        "\n",
        "if exists(path_to_model):\n",
        "    checkpoint = torch.load(path_to_model)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    criterion.load_state_dict(checkpoint['criterion_state_dict'])\n",
        "    curr_epoch = checkpoint['epoch'] + 1\n",
        "    train_loss_avg = checkpoint['loss']\n",
        "    train_accuracy = checkpoint['accuracy'] \n",
        "    try:\n",
        "        test_loss_avg = checkpoint['test_loss']\n",
        "        test_accuracy = checkpoint['test_accuracy']\n",
        "    except KeyError:\n",
        "        test_loss_avg = []\n",
        "        test_accuracy = []\n",
        "    del checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#graph print, directory creation and save execution cell\n",
        "def plotsave(modelname, lr, num_epochs, true, pred):\n",
        "    modelname = modelname\n",
        "    dirname = f\"{modelname} seq{seqlen} b{batchSize} lr{lr} ep{num_epochs}\"\n",
        "    dirpath = f'rslts/{dirname}'\n",
        "    os.mkdir(dirpath) \n",
        "\n",
        "    plot_loss(train_loss_avg,test_loss_avg,len(train_loss_avg),dirpath)\n",
        "    plot_accuracy(train_accuracy,test_accuracy,len(train_accuracy),dirpath)\n",
        "    print(confusion_matrix(true,pred))\n",
        "    print_confusion_matrix(true,pred,dirpath)\n",
        "\n",
        "    mvpath = f'{dirpath}/checkpoint.pt'\n",
        "    os.replace('ckpts/checkpoint.pt', mvpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "ORfzJ379czbb",
        "outputId": "2558c628-4b70-42c2-dbc4-fab634bd342a"
      },
      "outputs": [],
      "source": [
        "#model train execution cell\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(curr_epoch ,num_epochs+1):\n",
        "    msd, osd, csd, l, acc = train_epoch(epoch,num_epochs,train_loader,model,criterion,optimizer)\n",
        "    train_loss_avg.append(l)\n",
        "    train_accuracy.append(acc)\n",
        "    true,pred,tl,t_acc = test(epoch,model,valid_loader,criterion)\n",
        "    test_loss_avg.append(tl)\n",
        "    test_accuracy.append(t_acc)\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': msd.state_dict(),\n",
        "            'optimizer_state_dict': osd.state_dict(),\n",
        "            'criterion_state_dict': csd.state_dict(),\n",
        "            'loss': train_loss_avg,\n",
        "            'accuracy': train_accuracy,\n",
        "            'test_loss': test_loss_avg,\n",
        "            'test_accuracy': test_accuracy\n",
        "            }, path_to_model)\n",
        "plotsave(f\"moddedmodelblendshapesfacialparts 2dp0.4,0.4 relufacenet main_balanced_dset augmented sq{seqlen} im{im_size} rs{random_state}\",lr,num_epochs, true, pred)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dfproj_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
